# 챕터5 오차역전파법
가중치 매개 변수의 기울기를 효율적으로 계산하는 '오차역전파법'이 핵심 주제  
오차역전파법을 이해하는 방법에는 2가지가 있음.  
전자는 수식을 통한 이해  
후자는 계산 그래프를 통한 이해  
책에선 후자의 방법으로 학습할 것임.

계산 그래프  
\- 계산 과정을 그래프로 나타낸 것  
\- 자료구조로는 복수의 노드와 에지로 표현 (노드와 노드사이의 직선을 '에지'라고 표현)  
\- 계산 과정을 노드와 화살표로 표현  
\- 노드는 원(O)으로 표기, 원 안에 연산 내용을 적음  
\- 계산 결과를 화살표 위에 적어 각 노드의 계산 결과를 왼쪽에서 오른쪽으로 전해지게 함 (이런 방식을 '순전파'라고 함)

국소적 계산  
\- '자신과 직접 관계된 작은 범위'라는 뜻으로, 계산 그래프의 핵심적 특징  
\- 복잡한 계산을 분해하여 단순한 계산으로 구성할 수 있는 장점  

역전파  
\- 계산 그래프의 오른쪽에서 왼쪽으로 신호를 전파

덧셈 노드의 역전파: 1을 곱하기만 할 뿐, 입력된 값을 그대로 다음 노드로 보냄

곱셉 노드의 역전파  
\- 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄
\- 예를 들어, 순전파 때 x였다면 역전파에서는 y를, 순전파 때 y였다면 역전파에서는 x로 바꾼다는 의미  
\- 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해두어야 함

Relu 클래스에서 어려웠던 점  
\- forward 메소드에서 out[self.mask] = 0 문장의 작동이 이해가 안갔음  
self.mask는 불리안의 넘파이 배열 형태로, True가 입력되면 0을 대입, False가 입력되면 원래 원소값 출력

신경망에서 수행하는 작업은 '학습'과 '추론'  
학습할 땐 Softmax 함수가 필요, 추론할 땐 점수(Affine계층의 출력)가 필요함

---

## 신경망 학습의 절차   

전제  
\- 신경망에는 가중치와 편향이 있고, 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'
  
1단계 - 미니배치  
\- 훈련 데이터 중 일부를 무작위로 가져옵니다. 손실 함수 값을 줄이는 것이 목표  
  
**2단계 - 기울기 산출  
미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시  
(이번 챕터의 핵심인 오차역전파법이 2단계이다.)**  
  
3단계 - 매개변수 갱신  
\- 기울기 방향으로 아주 조금 갱신  
  
4단계 - 반복  
\- 1~3단계를 반복  

---
### two_layer_net 클래스의 인스턴트 변수
|인스턴트 변수||설명|
|---|---|---|
|params||딕셔너리 변수로, 신경망의 매개변수를 보관 <br>params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향<br>params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향|
|layers||순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관<br>layers['Affine1']}, layers['Relu1']}, layers['Affine2']와 같이 각 계층을 순서대로 유지|
|lastLayer||신경망의 마지막 계층<br>이 예에서는 SoftmaxWithLoss 계층|

---
---
### two_layer_net 클래스의 메서드
|인스턴트 변수||설명|
|---|---|---|
|\__init__(self, input_size, hidden_size, <br>output_size, weight_init_std)||초기화를 수행한다.<br>인수는 앞에서부터 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일|
|predict(self, x)||예측(추론)을 수행한다.<br>인수 x는 이미지 데이터|
|loss(self, x, t)||손실 함수의 값을 구한다.<br>인수 x는 이미지 데이터, t는 정답 레이블|
|accuracy(self, x, t)||정확도를 구한다.|
|numerical_gradient(self, x, t)||가중치 매개변수의 기울기를 수치 미분 방식으로 구한다|
|gradient(self, x, t)||가중치 매개변수의 기울기를 오차역전파법으로 구한다.|

---

OrderedDict  
\- 순서가 있는 딕셔너리 (추가한 순서를 기억한다는 뜻)  
\- 신경망의 계층을 OrderedDict에 보관하는 점이 중요  

기울기를 구하는 두가지 방법  
1.수치 미분을 써서 구하기  
2.해서적으로 수식을 풀어 구하기  

두번째 방법이 매개변수가 많아도 효율적으로 계산 가능  

수치 미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요  
\- 두 방식으로 구한 기울기가 일치함을 확인하는 작업을 '기울기 확인'이라고 함  

---
## 5장 정리
- 계산 그래프를 이용하여 계산 과정을 시각적으로 파악
- 계산 그래프의 노드는 국소적 계산으로 구성
- 순전파는 통상의 계산을 수행, 역전파는 각 노드의 미분을 구할 수 있음
- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산(오차역전파법)
- 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 있는지 없는지 확인할 수 있음(기울기 확인)