# 챕터6 학습 관련 기술들  

### **신경망 학습의 목적**  
-손실 함수의 값을 가능한 낮추는 매개변수를 찾는 것  
-이러한 문제를 푸는 것을 '최적화'  

### **확률적 경사 하강법(SGD)**
-매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복하여 최적의 값을 찾는 방법  
-갱신할 가중치 - 손실 함수의 기울기  
- 장점  
-단순하고, 구현이 쉬움  
- 단점  
-문제에 따라 비효율적인 경우가 있음  
-비등방성 함수(방향에 따라 성질이 달라지는 함수)에서는 탐색 경로가 비효율적

### **SGD의 단점을 개선해주는 대체 기법**
1. Momentum(모멘텀)
2. AdaGrad(아다그라드, Adaptive Gradient)
3. Adam(아담, Adaptive Moment Esimation)

### **모멘텀**
-모멘텀은 '운동량'을 뜻하는 단어  
-SGD와 비교하면 갱신 경로가 효율적임  

**학습률 감소**  
-신경망 학습에서는 학습률값이 중요  
-학습률을 정하는 효과적 기술로 학습률을 점차 줄여가는 방법  
-매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 방법을 발전시킨 것이 AdaGrad

### **AdaGrad**
-'각각의' 매개변수에 '맞춤형'값을 만들어 줌  
-개별 매개변수에 적응적으로(Adaptive) 학습률을 조정하면서 학습  
-SGD와 모멘텀 기법에 비하여 최솟값을 향해 효율적으로 움직임  
-***AdaGrad***를 개선한 기법으로 ***RMSProp***이 있음  

### **Adam**
-***모멘텀***은 공이 바닥을 구르는 듯한 움직임을 보임  
-***AdaGrad***는 매개변수의 원소마다 적응적으로 갱신 정도를 조정  
-***Adam***은 두 기법을 융합하는데서 출발한 기법  
-하이퍼파라미터의 '편향 보정'이 진행된다는 점도 특징

모든 문제에 항상 뛰어난 기법이란 것은 아직까진 없음  
풀어야 할 문제에 따라 위의 4개의 기법 중 적절한 기법을 선택하면 됨  

### **가중치 감소**  
-오버피팅을 억제해 범용 성능을 높이는 테크닉  
-가중치 매개변수의 값이 작아지도록 학습하는 방법  

### **기울기 소실**
-데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라짐  
-이러한 현상이 발생하면 뉴런을 여러개로 설정하는 의미가 없어짐  
-그렇기에 적절한 가중치 매개변수의 초깃값이 중요함  

### **Xavier(사비에르) 초깃값**  
-앞 계층의 노드가 n개라면 표준편차가 1/sqrt(n)인 분포 사용 **(sqrt = 제곱근)**  
-앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼짐  
-Sigmoid, Tanh 등의 S자 모양 곡선일 때 사용  

### **He(히) 초깃값**  
-ReLU에 특화된 초깃값  
-앞 계층의 노드가 n개일 때, 표준편차가 sqrt(2/n)  

### **배치 정규화**  
-각 층에서의 활성화값이 적당히 분포되도록 조정하는 것  
-배치 정규화의 장점  
- 학습을 빨리 진행할 수 있음(학습 속도 개선)  
- 초깃값에 크게 의존하지 않음  
- 오버피팅을 억제함(드롭아웃 등의 필요성 감소)  

### **오버 피팅**
-신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 적응하지 못하는 상태  
-즉 범용 성능이 떨어져, 학습하지 못한 데이터를 바르게 식별해내지 못함  
-주로 두 경우에 일어남  
1. 매개변수가 많고 표현력이 높은 모델
2. 훈련 데이터가 적은 경우

**L1노름**  
-각 원소의 절대값의 합

**L2노름**  
-각 원소의 제곱들의 합  

**Max노름**  
-각 원소의 절댓값 중 가장 큰 값에 해당

### **드롭 아웃**  
-뉴런을 임의로 삭제하면서 학습하는 방법  
-훈련 때 은닉층의 뉴런을 무작위로 골라 삭제  
-삭제된 뉴런은 신호를 전달하지 않게 됨  
-시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력  