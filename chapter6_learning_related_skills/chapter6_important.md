# 챕터6 학습 관련 기술들  

### **신경망 학습의 목적**  
-손실 함수의 값을 가능한 낮추는 매개변수를 찾는 것  
-이러한 문제를 푸는 것을 '최적화'  

### **확률적 경사 하강법(SGD)**
-매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복하여 최적의 값을 찾는 방법  
-갱신할 가중치 - 손실 함수의 기울기  
- 장점  
-단순하고, 구현이 쉬움  
- 단점  
-문제에 따라 비효율적인 경우가 있음  
-비등방성 함수(방향에 따라 성질이 달라지는 함수)에서는 탐색 경로가 비효율적

### **SGD의 단점을 개선해주는 대체 기법**
1. Momentum(모멘텀)
2. AdaGrad(아다그라드, Adaptive Gradient)
3. Adam(아담, Adaptive Moment Esimation)

### **모멘텀**
-모멘텀은 '운동량'을 뜻하는 단어  
-SGD와 비교하면 갱신 경로가 효율적임  

**학습률 감소**  
-신경망 학습에서는 학습률값이 중요  
-학습률을 정하는 효과적 기술로 학습률을 점차 줄여가는 방법  
-매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 방법을 발전시킨 것이 AdaGrad

### **AdaGrad**
-'각각의' 매개변수에 '맞춤형'값을 만들어 줌  
-개별 매개변수에 적응적으로(Adaptive) 학습률을 조정하면서 학습  
-SGD와 모멘텀 기법에 비하여 최솟값을 향해 효율적으로 움직임  
-***AdaGrad***를 개선한 기법으로 ***RMSProp***이 있음  

### **Adam**
-***모멘텀***은 공이 바닥을 구르는 듯한 움직임을 보임  
-***AdaGrad***는 매개변수의 원소마다 적응적으로 갱신 정도를 조정  
-***Adam***은 두 기법을 융합하는데서 출발한 기법  
-하이퍼파라미터의 '편향 보정'이 진행된다는 점도 특징

모든 문제에 항상 뛰어난 기법이란 것은 아직까진 없음  
풀어야 할 문제에 따라 위의 4개의 기법 중 적절한 기법을 선택하면 됨  