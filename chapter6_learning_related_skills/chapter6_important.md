# 챕터6 학습 관련 기술들  

### **신경망 학습의 목적**  
\- 손실 함수의 값을 가능한 낮추는 매개변수를 찾는 것  
\- 이러한 문제를 푸는 것을 '최적화'  

### **확률적 경사 하강법(SGD)**
\- 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복하여 최적의 값을 찾는 방법  
\- 갱신할 가중치 - 손실 함수의 기울기  
- 장점  
-단순하고, 구현이 쉬움  
- 단점  
-문제에 따라 비효율적인 경우가 있음  
-비등방성 함수(방향에 따라 성질이 달라지는 함수)에서는 탐색 경로가 비효율적

### **SGD의 단점을 개선해주는 대체 기법**
1. Momentum(모멘텀)
2. AdaGrad(아다그라드, Adaptive Gradient)
3. Adam(아담, Adaptive Moment Esimation)

### **모멘텀**
\- 모멘텀은 '운동량'을 뜻하는 단어  
\- SGD와 비교하면 갱신 경로가 효율적임  

**학습률 감소**  
\- 신경망 학습에서는 학습률값이 중요  
\- 학습률을 정하는 효과적 기술로 학습률을 점차 줄여가는 방법  
\- 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 방법을 발전시킨 것이 AdaGrad

### **AdaGrad**
\- '각각의' 매개변수에 '맞춤형'값을 만들어 줌  
\- 개별 매개변수에 적응적으로(Adaptive) 학습률을 조정하면서 학습  
\- SGD와 모멘텀 기법에 비하여 최솟값을 향해 효율적으로 움직임  
\- ***AdaGrad***를 개선한 기법으로 ***RMSProp***이 있음  

### **Adam**
\- ***모멘텀***은 공이 바닥을 구르는 듯한 움직임을 보임  
\- ***AdaGrad***는 매개변수의 원소마다 적응적으로 갱신 정도를 조정  
\- ***Adam***은 두 기법을 융합하는데서 출발한 기법  
\- 하이퍼파라미터의 '편향 보정'이 진행된다는 점도 특징

모든 문제에 항상 뛰어난 기법이란 것은 아직까진 없음  
풀어야 할 문제에 따라 위의 4개의 기법 중 적절한 기법을 선택하면 됨  

### **가중치 감소**  
\- 오버피팅을 억제해 범용 성능을 높이는 테크닉  
\- 가중치 매개변수의 값이 작아지도록 학습하는 방법  

### **기울기 소실**
\- 데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라짐  
\- 이러한 현상이 발생하면 뉴런을 여러개로 설정하는 의미가 없어짐  
\- 그렇기에 적절한 가중치 매개변수의 초깃값이 중요함  

### **Xavier(사비에르) 초깃값**  
\- 앞 계층의 노드가 n개라면 표준편차가 1/sqrt(n)인 분포 사용 **(sqrt = 제곱근)**  
\- 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼짐  
\- Sigmoid, Tanh 등의 S자 모양 곡선일 때 사용  

### **He(히) 초깃값**  
\- ReLU에 특화된 초깃값  
\- 앞 계층의 노드가 n개일 때, 표준편차가 sqrt(2/n)  

### **배치 정규화**  
\- 각 층에서의 활성화값이 적당히 분포되도록 조정하는 것  
\- 배치 정규화의 장점  
- 학습을 빨리 진행할 수 있음(학습 속도 개선)  
- 초깃값에 크게 의존하지 않음  
- 오버피팅을 억제함(드롭아웃 등의 필요성 감소)  

### **오버 피팅**
\- 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 적응하지 못하는 상태  
\- 즉 범용 성능이 떨어져, 학습하지 못한 데이터를 바르게 식별해내지 못함  
\- 주로 두 경우에 일어남  
1. 매개변수가 많고 표현력이 높은 모델
2. 훈련 데이터가 적은 경우

**L1노름**  
\- 각 원소의 절대값의 합

**L2노름**  
\- 각 원소의 제곱들의 합  

**Max노름**  
\- 각 원소의 절댓값 중 가장 큰 값에 해당

### **드롭 아웃**  
\- 뉴런을 임의로 삭제하면서 학습하는 방법  
\- 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제  
\- 삭제된 뉴런은 신호를 전달하지 않게 됨  
\- 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력  

### **적절한 하이퍼파라미터 값 찾기**  

**검증 데이터**  
\- 하이퍼파라미터의 성능을 평가할 때는 '시험 데이터'를 사용하면 안됨  
\- 범용성을 평가할 데이터에 맞춰져버리기 때문에 시험 데이터의 의미를 잃음  
\- 하이퍼파라미터 전용 확인 데이터인 '검증 데이터'가 필요함  
- 훈련 데이터: 매개변수 학습
- 검증 데이터: 하이퍼파라미터 성능 평가
- 시험 데이터: 신경망의 범용 성능 평가  

**하이퍼파라미터 최적화**  
\- 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 샘플링 후, 그 값으로 정확도를 평가  
\- '최적 값'의 범위를 좁혀가는 것  
\- '10의 거듭제곱'단위로 범위를 지정(이를 '로그 스케일'로 지정한다고 함)  
- 0단계: 하이퍼파라미터 값의 범위를 설정(로그 스케일)
- 1단계: 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출
- 2단계: 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 검증하고, 검증 데이터로 정확도를 평가(에폭은 작게 설정)
- 3단계: 1단계와 2단계를 특정 횟수(100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁혀나감  

---
## 6장 정리
- 매개변수 갱신 방법에는 '확률정 경사 하강법(SGD)', '모멘텀', 'AdaGrad', 'Adam' 등이 있음
- 가중치의 초깃값으로는 'Xavier 초깃값'과 'He 초깃값'이 효과적
- 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값 영향을 덜 받음
- 오버피팅을 억제하는 정규화 기술로는 '가중치 감소'와 '드롭아웃'이 있음
- 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적
